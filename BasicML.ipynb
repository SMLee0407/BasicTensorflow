{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b578aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0l    2.4520l    0.3760l 45.660004\n",
      "   10l    1.1036l    0.0034l  0.206336\n",
      "   20l    1.0128l   -0.0209l  0.001026\n",
      "   30l    1.0065l   -0.0218l  0.000093\n",
      "   40l    1.0059l   -0.0212l  0.000083\n",
      "   50l    1.0057l   -0.0205l  0.000077\n",
      "   60l    1.0055l   -0.0198l  0.000072\n",
      "   70l    1.0053l   -0.0192l  0.000067\n",
      "   80l    1.0051l   -0.0185l  0.000063\n",
      "   90l    1.0050l   -0.0179l  0.000059\n",
      "  100l    1.0048l   -0.0173l  0.000055\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #linear regresstion (continuous)\n",
    "x_data = [1,2,3,4,5]\n",
    "y_data = [1,2,3,4,5]\n",
    "W = tf.Variable(2.9)\n",
    "b = tf.Variable(0.5)\n",
    "learning_rate = 0.01\n",
    "for i in range(100+1):\n",
    "    with tf.GradientTape() as tape: #tape에 내용 전달\n",
    "        hypothesis = W*x_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) #MSE\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b]) #gradient 계산\n",
    "    W.assign_sub(learning_rate*W_grad) #빼고 할당\n",
    "    b.assign_sub(learning_rate*b_grad)\n",
    "    if i%10 == 0:\n",
    "        print(\"{:5}l{:10.4f}l{:10.4f}l{:10.6f}\".format(i, W.numpy(), b.numpy(), cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "642f1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(5.00667, shape=(), dtype=float32)\n",
      "tf.Tensor(2.4946702, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(W*5+b)\n",
    "print(W*2.5+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56568c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.000 l   74.66667\n",
      "-2.429 l   54.85714\n",
      "-1.857 l   38.09524\n",
      "-1.286 l   24.38095\n",
      "-0.714 l   13.71429\n",
      "-0.143 l    6.09524\n",
      " 0.429 l    1.52381\n",
      " 1.000 l    0.00000\n",
      " 1.571 l    1.52381\n",
      " 2.143 l    6.09524\n",
      " 2.714 l   13.71429\n",
      " 3.286 l   24.38095\n",
      " 3.857 l   38.09524\n",
      " 4.429 l   54.85714\n",
      " 5.000 l   74.66667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,3])\n",
    "def cost_func(W,X,Y):\n",
    "    hypothesis = X*W\n",
    "    return tf.reduce_mean(tf.square(hypothesis-Y))\n",
    "W_values = np.linspace(-3,5,num=15)\n",
    "cost_values = []\n",
    "for feed_W in W_values:\n",
    "    curr_cost = cost_func(feed_W, X, Y)\n",
    "    cost_values.append(curr_cost)\n",
    "    print(\"{:6.3f} l {:10.5f}\".format(feed_W, curr_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b73e6fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0l18829.7812l 47.348293\n",
      "   10l 3959.8606l 22.254507\n",
      "   20l  832.7498l 10.746942\n",
      "   30l  175.1254l  5.469775\n",
      "   40l   36.8285l  3.049760\n",
      "   50l    7.7449l  1.939983\n",
      "   60l    1.6287l  1.431060\n",
      "   70l    0.3425l  1.197676\n",
      "   80l    0.0720l  1.090651\n",
      "   90l    0.0151l  1.041571\n",
      "  100l    0.0032l  1.019064\n",
      "  110l    0.0007l  1.008742\n",
      "  120l    0.0001l  1.004009\n",
      "  130l    0.0000l  1.001839\n",
      "  140l    0.0000l  1.000843\n",
      "  150l    0.0000l  1.000387\n",
      "  160l    0.0000l  1.000178\n",
      "  170l    0.0000l  1.000081\n",
      "  180l    0.0000l  1.000037\n",
      "  190l    0.0000l  1.000017\n",
      "  200l    0.0000l  1.000008\n",
      "  210l    0.0000l  1.000004\n",
      "  220l    0.0000l  1.000002\n",
      "  230l    0.0000l  1.000001\n",
      "  240l    0.0000l  1.000001\n",
      "  250l    0.0000l  1.000001\n",
      "  260l    0.0000l  1.000001\n",
      "  270l    0.0000l  1.000001\n",
      "  280l    0.0000l  1.000001\n",
      "  290l    0.0000l  1.000001\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0) #처음 실행 시 랜덤으로 나오는 W 고정\n",
    "X = [1., 2., 3., 4.]\n",
    "Y = [1., 2., 3., 4.]\n",
    "W = tf.Variable(tf.random.normal([1], -100., 100.)) #W의차원, 범위\n",
    "for step in range(300):\n",
    "    hypothesis = W*X\n",
    "    cost = tf.reduce_mean(tf.square(hypothesis-Y)) #차원축소\n",
    "    alpha = 0.01\n",
    "    gradient = tf.reduce_mean(tf.multiply(tf.multiply(W,X) - Y, X))\n",
    "    descent = W - tf.multiply(alpha, gradient)\n",
    "    W.assign(descent)\n",
    "    if step%10 == 0:\n",
    "        print(\"{:5}l{:10.4f}l{:10.6f}\".format(step, cost.numpy(), W.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f4c8e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0l 1798.2894\n",
      "  100l    2.2888\n",
      "  200l    2.0632\n",
      "  300l    2.0587\n",
      "  400l    2.0542\n",
      "  500l    2.0498\n",
      "  600l    2.0453\n",
      "  700l    2.0409\n",
      "  800l    2.0366\n",
      "  900l    2.0322\n",
      " 1000l    2.0279\n",
      " 1100l    2.0236\n",
      " 1200l    2.0194\n",
      " 1300l    2.0151\n",
      " 1400l    2.0108\n",
      " 1500l    2.0066\n",
      " 1600l    2.0024\n",
      " 1700l    1.9982\n",
      " 1800l    1.9940\n",
      " 1900l    1.9899\n",
      " 2000l    1.9857\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0) #처음 실행 시 랜덤으로 나오는 W 고정 #multivariable linear regresstion\n",
    "data = np.array([\n",
    "    [73., 80., 75., 152.],\n",
    "    [93., 88., 93., 185.],\n",
    "    [89., 91., 90., 180.],\n",
    "    [96., 98., 100., 196.],\n",
    "    [73., 66., 70., 142.]\n",
    "], dtype=np.float32)\n",
    "X = data[:, :-1]\n",
    "Y = data[:,[-1]]\n",
    "W = tf.Variable(tf.random.normal([3,1])) #W의차원\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b #XW+b\n",
    "n_epochs = 2000\n",
    "learning_rate = 0.000001\n",
    "for i in range(n_epochs+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean(tf.square(predict(X)-Y))\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    W.assign_sub(learning_rate*W_grad)\n",
    "    b.assign_sub(learning_rate*b_grad)            \n",
    "    if i%100 == 0:\n",
    "        print(\"{:5}l{:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e968549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #visualization\n",
    "x_train = [[1.,2.],[2.,3.],[3.,1.],[4.,3.],[5.,3.],[6.,2.]]\n",
    "y_train = [[0.],[0.],[0.],[1.],[1.],[1.]]\n",
    "x_test = [[5.,2.]]\n",
    "y_test = [[1.]]\n",
    "x1 = [x[0] for x in x_train]\n",
    "x2 = [x[1] for x in x_train]\n",
    "colors = [int(y[0]%3) for y in y_train]\n",
    "plt.scatter(x1,x2,c=colors, marker='^')\n",
    "plt.scatter(x_test[0][0], x_test[0][1], c='red')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "133b846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, Loss: 0.6931\n",
      "Iter: 100, Loss: 0.5781\n",
      "Iter: 200, Loss: 0.5352\n",
      "Iter: 300, Loss: 0.5056\n",
      "Iter: 400, Loss: 0.4840\n",
      "Iter: 500, Loss: 0.4673\n",
      "Iter: 600, Loss: 0.4537\n",
      "Iter: 700, Loss: 0.4421\n",
      "Iter: 800, Loss: 0.4320\n",
      "Iter: 900, Loss: 0.4229\n",
      "Iter: 1000, Loss: 0.4145\n",
      "Test Result = [[1]]\n",
      "Testset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) #Binary classification with logistic regresstion (discrete)\n",
    "W = tf.Variable(tf.zeros([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.zeros([1]), name = 'bias')\n",
    "\n",
    "def logistic_regression(features): #sigmoid 함수 (0부터 1까지)\n",
    "    hypothesis = tf.divide(1., 1+tf.exp(-(tf.matmul(features,W)+b)))\n",
    "    return hypothesis\n",
    "def loss_fn(hypothesis, labels): #cost (for convex)\n",
    "    cost = -tf.reduce_mean(labels*tf.math.log(hypothesis)+(1-labels)*tf.math.log(1-hypothesis))\n",
    "    return cost\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "def accuracy_fn(hypothesis, labels):\n",
    "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) #True면 1, False면 0 반환\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))\n",
    "    return accuracy\n",
    "def grad(features, labels): #grad 구하기\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = logistic_regression(features)\n",
    "        loss_value = loss_fn(hypothesis, labels)\n",
    "    return tape.gradient(loss_value, [W,b])\n",
    "EPOCHS = 1001\n",
    "for step in range(EPOCHS):\n",
    "        for features, labels in iter(dataset.batch(len(x_train))): #실제 크기 6, 한번\n",
    "            hypothesis = logistic_regression(features)\n",
    "            grads = grad(features, labels)\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) #변수조정\n",
    "            if step %100 == 0:\n",
    "                print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(hypothesis, labels)))\n",
    "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
    "print(\"Test Result = {}\".format(tf.cast(logistic_regression(x_test) > 0.5, dtype=tf.int32)))\n",
    "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e68484d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 3)\n",
      "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[ 0.7706481 ,  0.37335402, -0.05576323],\n",
      "       [ 0.00358377, -0.5898363 ,  1.5702795 ],\n",
      "       [ 0.2460895 , -0.09918973,  1.4418385 ],\n",
      "       [ 0.3200988 ,  0.526784  , -0.7703731 ]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-1.3080608 , -0.13253094,  0.5513761 ], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(777)  # for reproducibility #Multiple classification with softmax\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1], #one-hot\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]] \n",
    "#convert into numpy and float format\n",
    "x_data = np.asarray(x_data, dtype=np.float32)\n",
    "y_data = np.asarray(y_data, dtype=np.float32)\n",
    "nb_classes = 3 #class의 개수입니다.\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "variables = [W, b]\n",
    "print(W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6246b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 2.849417\n",
      "Loss at epoch 100: 0.684151\n",
      "Loss at epoch 200: 0.613813\n",
      "Loss at epoch 300: 0.558204\n",
      "Loss at epoch 400: 0.508306\n",
      "Loss at epoch 500: 0.461059\n",
      "Loss at epoch 600: 0.415072\n",
      "Loss at epoch 700: 0.369636\n",
      "Loss at epoch 800: 0.324533\n",
      "Loss at epoch 900: 0.280720\n",
      "Loss at epoch 1000: 0.246752\n",
      "Loss at epoch 1100: 0.232798\n",
      "Loss at epoch 1200: 0.221645\n",
      "Loss at epoch 1300: 0.211476\n",
      "Loss at epoch 1400: 0.202164\n",
      "Loss at epoch 1500: 0.193606\n",
      "Loss at epoch 1600: 0.185714\n",
      "Loss at epoch 1700: 0.178415\n",
      "Loss at epoch 1800: 0.171645\n",
      "Loss at epoch 1900: 0.165351\n",
      "Loss at epoch 2000: 0.159483\n"
     ]
    }
   ],
   "source": [
    "def hypothesis(X): #확률값으로 변환\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "def cost_fn(X, Y): \n",
    "    logits = hypothesis(X)\n",
    "    cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1) #cross entropy 산출\n",
    "    cost_mean = tf.reduce_mean(cost)\n",
    "    return cost_mean\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "def fit(X, Y, epochs=2000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))       \n",
    "fit(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9c88a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00112886 0.08154673 0.9173244 ]], shape=(1, 3), dtype=float32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[2.1975952e-06 1.2331170e-03 9.9876475e-01]\n",
      " [1.1288594e-03 8.1546687e-02 9.1732442e-01]\n",
      " [2.2205539e-07 1.6418624e-01 8.3581358e-01]\n",
      " [6.3921816e-06 8.5045439e-01 1.4953922e-01]\n",
      " [2.6150808e-01 7.2644734e-01 1.2044534e-02]\n",
      " [1.3783246e-01 8.6214006e-01 2.7417480e-05]\n",
      " [7.4242145e-01 2.5754160e-01 3.6978410e-05]\n",
      " [9.2197549e-01 7.8023903e-02 6.0005692e-07]], shape=(8, 3), dtype=float32)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]] 테스트\n",
    "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
    "a = hypothesis(sample_data)\n",
    "print(a)\n",
    "print(tf.argmax(a, 1)) #index: 2 (최대값 인덱스 반환)\n",
    "\n",
    "b = hypothesis(x_data)\n",
    "print(b)\n",
    "print(tf.argmax(b, 1))\n",
    "print(tf.argmax(y_data, 1)) # matches with y_data 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3a0355b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 7)\n",
      "Steps: 1 Loss: 3.635028839111328, Acc: 0.1683168262243271\n",
      "Steps: 100 Loss: 0.5194158554077148, Acc: 0.7920792102813721\n",
      "Steps: 200 Loss: 0.31850093603134155, Acc: 0.9108911156654358\n",
      "Steps: 300 Loss: 0.23534876108169556, Acc: 0.9405940771102905\n",
      "Steps: 400 Loss: 0.18872138857841492, Acc: 0.9504950642585754\n",
      "Steps: 500 Loss: 0.1584603488445282, Acc: 0.9504950642585754\n",
      "Steps: 600 Loss: 0.13703754544258118, Acc: 0.9900990128517151\n",
      "Steps: 700 Loss: 0.12098980695009232, Acc: 0.9900990128517151\n",
      "Steps: 800 Loss: 0.10847963392734528, Acc: 1.0\n",
      "Steps: 900 Loss: 0.09843039512634277, Acc: 1.0\n",
      "Steps: 1000 Loss: 0.09016558527946472, Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(777)\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32) #csv data load\n",
    "x_data = xy[:, 0:-1] #vectorization\n",
    "y_data = xy[:, -1]\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)\n",
    "print(x_data.shape, Y_one_hot.shape)\n",
    "#Weight and bias setting\n",
    "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight')\n",
    "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias') #', '은 알아서 넣어짐\n",
    "variables = [W, b]\n",
    "def logit_fn(X): #선형변환 logit\n",
    "    return tf.matmul(X, W) + b\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(logit_fn(X))\n",
    "def cost_fn(X, Y):\n",
    "    logits = logit_fn(X)\n",
    "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, \n",
    "                                                      from_logits=True)    #logit을 이용해 바로 cost vector 구하기\n",
    "    cost = tf.reduce_mean(cost_i)    \n",
    "    return cost\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1) #예측레이블\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "def fit(X, Y, epochs=1000, verbose=100):\n",
    "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy() \n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc40089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
